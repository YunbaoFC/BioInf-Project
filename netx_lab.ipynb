{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install networkx scipy matplotlib pandas datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph(name = \"very_cool_graph\")\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding nodes and edges to the graph\n",
    "\n",
    "G.add_node(1)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding several nodes\n",
    "### G.nodes is not sorted by value, but by order of addition.\n",
    "\n",
    "nodes_to_add = [2,3,4,5,6,7,8,11,20,9]\n",
    "G.add_nodes_from(nodes_to_add)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We add edges to the graph. The edges must be lists of tuples, like: [(1,2), (2,3), ...]\n",
    "### each tuple indicates the couple of nodes to be connected\n",
    "\n",
    "edges_to_add = [(1,2),(2,3),(3,4)]\n",
    "G.add_edges_from(edges_to_add)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note that the class Graph handles undirected graphs. In fact edge (1,2) and (2,1) have the same meaning.\n",
    "### If you want to work with directed graphs, use the class DiGraph.\n",
    "\n",
    "# We can examine the elements of our graph\n",
    "\n",
    "print(\"list of nodes: \",list(G.nodes))\n",
    "\n",
    "print(\"list of edges: \",list(G.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can also see the neighbors of a node with G.adj = adjacent\n",
    "\n",
    "nd = 2\n",
    "\n",
    "print(\"neighbors of node\",nd,\":\",list(G.adj[nd]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing a node from a graph --> the edges involved are also removed\n",
    "\n",
    "G.remove_node(2)\n",
    "print(G)\n",
    "print(\"nodes:\",list(G.nodes))\n",
    "print(\"edges:\",list(G.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's add again node 2 to the graph\n",
    "\n",
    "G.add_node(2)\n",
    "print(G)\n",
    "print(\"nodes:\",list(G.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "### G.nodes is not sorted by value, but by order of addition.\n",
    "### NetworkX does not automatically reorder nodes (either by number or label).\n",
    "\n",
    "print(sorted(G.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges can also have a weight\n",
    "\n",
    "weighted_edges = [(1,2,0.5), (2,3,0.2), (21,22,0.5)] #--> if the nodes are not present, they are added\n",
    "G.add_weighted_edges_from(weighted_edges)\n",
    "print(G)\n",
    "\n",
    "print(\"nodes:\",list(G.nodes))\n",
    "print(\"edges:\",list(G.edges))\n",
    "\n",
    "for e in G.edges:\n",
    "    print(e, G.get_edge_data(e[0],e[1])) # Returns the attribute dictionary associated with edge (u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We add attributes to nodes\n",
    "\n",
    "attr = {3: {\"color\": \"red\", \"size\": 12}, 4: {\"color\": \"blue\"}}\n",
    "\n",
    "# Create an attr dictionary that associates attributes with the nodes of the graph.\n",
    "# The key is the node ID (3 and 4),\n",
    "# The value is another dictionary with the properties (attributes) of that node.\n",
    "\n",
    "nx.set_node_attributes(G,attr)\n",
    "\n",
    "# Adds (or updates) the attributes of the nodes in graph G according to the attr dictionary.\n",
    "# Now node 3 will have colour=\"red\" and size=3\n",
    "# Node 4 will have colour=\"blue\"\n",
    "# NetworkX stores this data in G.nodes[data=True].\n",
    "\n",
    "print(\"all attributed attributes:\",nx.get_node_attributes(G, \"color\")) # attribute \"color\" only\n",
    "\n",
    "# extracts the values of a certain attribute\n",
    "\n",
    "no = 3\n",
    "\n",
    "print(\"attr of node\",no,\":\",G.nodes[no])\n",
    "print(\"attr color of node\",no,\":\",G.nodes[no]['color'])\n",
    "print(\"attr size of node\",no,\":\",G.nodes[no]['size'])\n",
    "\n",
    "# Display all attributes associated with that node, like a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We add attributes to edges\n",
    "\n",
    "#the edge weight is considered as an attribute\n",
    "\n",
    "print(G.edges[(1,2)])\n",
    "\n",
    "edge_attr = {(1,2): {\"color\": \"red\", \"visited\":True}}\n",
    "print(nx.set_edge_attributes(G, edge_attr))\n",
    "print(G.edges[(1,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can obtain the adjacency matrix of the graph. \n",
    "# Note that this compact representation saves nodes only if they are connected by an edge \n",
    "#(showing the weight of the edge, if available)\n",
    "\n",
    "adj = nx.adjacency_matrix(G)\n",
    "print(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To visualize the matrix, we need to render it dense (not suitable for large graphs!). \n",
    "# Note that what matters here are the indices of the nodes (not the names).\n",
    "\n",
    "print(adj.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Directed Graphs\n",
    "\n",
    "DG = nx.DiGraph()\n",
    "edges_to_add = [(1,2),(2,3),(3,4),(2,1)]\n",
    "DG.add_edges_from(edges_to_add)\n",
    "print(DG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing Graphs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"undirected graph\")\n",
    "\n",
    "nx.draw(G, with_labels=True, font_weight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"directed graph\")\n",
    "\n",
    "nx.draw(DG, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Union, Intersection and Difference of Networks\n",
    "# Now that we know some basics on networkX and how to visualize a graph, \n",
    "# we can now see how to perform operations on graphs. \n",
    "# We show now union, intersections and difference using random graphs, \n",
    "# we will later load a graph from file in order to perform other operations \n",
    "# and centrality measures computations.\n",
    "\n",
    "# NetworkX has fucntions to generate graphs according to known models. \n",
    "# We use the Barabasi-Albert preferential attachment.\n",
    "\n",
    "ba = nx.barabasi_albert_graph(20, 2)\n",
    "print(ba)\n",
    "\n",
    "print(\"BA graph\")\n",
    "\n",
    "nx.draw(ba, with_labels=True, font_weight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Another possibility is to generate a Erdos Renyi graph.\n",
    "\n",
    "er = nx.erdos_renyi_graph(20, 0.2) \n",
    "\n",
    "#-->The second parameter here (2) is the number of arcs that each new node adds when it enters the graph.\n",
    "\n",
    "print(er)\n",
    "\n",
    "print(\"ER graph\")\n",
    "\n",
    "nx.draw(er, with_labels=True, font_weight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph union \n",
    "\n",
    "# unisng nx.union() graphs must be disjoint (no nodes in common). \n",
    "# If this is not the case one can use the parameter *rename* to renamed the nodes \n",
    "# such that the graphs become disjoint. It is ok for this tutorial puposes.\n",
    "\n",
    "### U = nx.union(ba, er, rename=(\"ba\", \"er\")) # --> if a node has the same name in both graphs → conflict\n",
    "# because NetworkX does not know which graph the attributes come from.\n",
    "\n",
    "# better use nx.compose \n",
    "\n",
    "# nx.compose(G1, G2) does something different:\n",
    "# •    it merges nodes with the same name\n",
    "# •    if a node is in both, it is merged\n",
    "# •    if an arc appears in both, only one remains\n",
    "# •    attributes are merged (in case of conflict? the last graph prevails)\n",
    "\n",
    "U = nx.compose(ba, er)\n",
    "\n",
    "U.name = \"u_ba-er\"\n",
    "\n",
    "print(U)\n",
    "nx.draw(U, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphs intersection\n",
    "# It returs a graphs that contains only the nodes and the edges that exist in both graphs. \n",
    "#It is interesting to explore what two random generated graphs share\n",
    "\n",
    "I = nx.intersection(ba, er)\n",
    "\n",
    "I.name = \"i_ba-er\"\n",
    "\n",
    "print(I)\n",
    "print(\"edges only existing in both graphs:\",I.edges())\n",
    "nx.draw(I, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph difference\n",
    "# Returns a new graph that contains the edges that exist in the first graph but not in the second\n",
    "# graph order is important\n",
    "# NOTE: nodes in the two graphs must be the same\n",
    "\n",
    "D12 = nx.difference(ba, er)\n",
    "\n",
    "D21 = nx.difference(er, ba) #try also the opposite, which is different\n",
    "\n",
    "print(D12)\n",
    "print(\"edges existing in graph A but not in B:\",len(D12.edges()),\":\",D12.edges())\n",
    "nx.draw(D12, with_labels=True, font_weight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(D21)\n",
    "print(\"edges existing in graph B but not in A:\",len(D21.edges()),\":\",D21.edges())\n",
    "nx.draw(D21, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading a graph\n",
    "# For the rest of the lab we will load and use a reduced version of the Biogrid network\n",
    "#containing known protein-protein interactions\n",
    "\n",
    "interactome = open(\"Biogrid_homo_1000_edgelist.txt\", 'rb')\n",
    "\n",
    "G=nx.read_edgelist(interactome)\n",
    "\n",
    "print(\"Number of nodes in the interactome: \" + str(len(G)) + '\\n')\n",
    "\n",
    "print(\"Number of edges in the interactome: \" +str(G.number_of_edges()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing self loops\n",
    "\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "print(\"Number of edges  with no self loops in the interactome: \" +str(G.number_of_edges()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract connected components and sorting them from the largest (key: number of nodes) to the smallest\n",
    "\n",
    "print(\"Extracing largest connected component (LCC)...\" + '\\n')\n",
    "\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "\n",
    "# G.subgraph returns a graph H that contains:\n",
    "# 1. only the nodes specified in nodes\n",
    "# 2. only the arcs connecting those nodes that already existed in G\n",
    "\n",
    "\n",
    "LCC = G.subgraph(Gcc[0])\n",
    "\n",
    "print(\"Number of nodes in the LCC: \" + str(len(LCC)) + '\\n')\n",
    "\n",
    "print(\"Number of edges in the LCC: \" +str(LCC.number_of_edges()) + '\\n')\n",
    "\n",
    "nx.draw(LCC, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path between two nodes\n",
    "# We can both check if the path exist and compute the shortest path. \n",
    "#We are working with a connected component, so we expect the path to exist.\n",
    "\n",
    "nx.has_path(LCC, \"ADAM15\", \"TNKS\")\n",
    "\n",
    "path = nx.shortest_path(LCC, \"ADAM15\", \"TNKS\")\n",
    "\n",
    "print(len(path),\":\",path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.has_path(LCC, \"ADAM15\", \"AATF\")\n",
    "\n",
    "path = nx.shortest_path(LCC, \"ADAM15\", \"AATF\")\n",
    "\n",
    "print(len(path),\":\",path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes and edges\n",
    "num_nodes = len(path)\n",
    "num_edges = len(path) - 1  # edges are always one less than nodes\n",
    "\n",
    "print(f\"Nodes in the path ({num_nodes}): {path}\")\n",
    "print(f\"Edges in the path ({num_edges}):\")\n",
    "\n",
    "# Print each edge in the path\n",
    "for i in range(num_edges):\n",
    "    print(f\"{path[i]} — {path[i+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest path length in terms of edges DIRECT FUNCTION shortest_path_length\n",
    "\n",
    "num_edges = nx.shortest_path_length(LCC, source=\"ADAM15\", target=\"AATF\")\n",
    "\n",
    "print(\"Number of edges in the shortest path:\", num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and visualize a subgraph given a node and its first neighbors\n",
    "\n",
    "# nod = \"AP1G2\"\n",
    "nod = \"XRN1\"\n",
    "# nod = \"SNX9\"\n",
    "\n",
    "nodes_to_viz = [nod] + list(LCC.adj[nod]) # --> .adj[n] reports all nodes connected to n\n",
    "\n",
    "print(\"node:\",nod,\";\",\"neighbors of\",nod,\":\",list(LCC.adj[nod]))\n",
    "\n",
    "nodes_to_viz\n",
    "\n",
    "sub_LCC = LCC.subgraph(nodes_to_viz) # --> will include all edges between these selected nodes\n",
    "\n",
    "color_map = ['red' if node == nod else 'green' for node in sub_LCC]\n",
    "\n",
    "nx.draw(sub_LCC, with_labels=True, node_color = color_map, font_weight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph measures\n",
    "# We now explore some global and local graph measures. \n",
    "# Global measures are referred to properties related to the whole graph\n",
    "# while local measures are centrality measures computed for nodes (or edges)\n",
    "# Those measures are extremely useful when looking for important/central nodes in a given graph\n",
    "# There are many centrality measures, each one with a different biological interpretation. \n",
    "\n",
    "### Gloabal measures\n",
    "\n",
    "# Compute **average shortest path length**: \n",
    "# compute all the shortest path from all nodes to all others and average:\n",
    "\n",
    "apl = nx.average_shortest_path_length(LCC)\n",
    "\n",
    "print(\"Avg shortest path length: \", apl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average degree: average number of connected edges for any node \n",
    "# (relevant in random graphs, not relevant in scale-free graphs)\n",
    "\n",
    "degrees = LCC.degree()\n",
    "\n",
    "print(degrees,\"\\n\")\n",
    "\n",
    "avg_degree = sum(dict(degrees).values())/LCC.number_of_nodes()\n",
    "\n",
    "print(\"Avg degree: \", avg_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Average clustering coefficient**\n",
    "#this measures the extent to which nodes tend to cluster together in the graph\n",
    "\n",
    "avg_cluster = nx.average_clustering(LCC)\n",
    "\n",
    "print(\"Avg clustering coefficient: \", avg_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Eccentricy: maximun among all minimum distances between a vertex to all other vertices.\n",
    "# Example:\n",
    "# d(A,B)=1, d(A,C)=2, d(A,D)=3\n",
    "# ecc(A) = max(1,2,3) = 3\n",
    "\n",
    "ecc = nx.eccentricity(LCC)\n",
    "print(\"eccentricity: \", ecc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Radius: minimum graph eccentricity of any graph vertex in a graph.\n",
    "\n",
    "radius = nx.radius(LCC)\n",
    "\n",
    "print(\"radius: \", radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Diameter: maximum eccentricity of any vertex in the graph.\n",
    "\n",
    "diameter = nx.diameter(LCC)\n",
    "\n",
    "print(\"diameter: \", diameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Center: set of nodes with minumun eccentricity (i.e. radius).\n",
    "\n",
    "center = nx.center(LCC)\n",
    "\n",
    "print(\"center: \", center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(LCC.subgraph(center), with_labels=True, font_weight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Periphery: set of nodes with eccentricity equal to the diameter. \n",
    "\n",
    "per = nx.periphery(LCC)\n",
    "\n",
    "print(\"periphery: \", per)\n",
    "\n",
    "nx.draw(LCC.subgraph(per), with_labels=True, font_weight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Density\n",
    "\n",
    "density = nx.density(LCC)\n",
    "\n",
    "print(\"density: \", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computing local centrality metrics\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# computing degree\n",
    "\n",
    "print(\"Computing centralities with networkx...\" + '\\n')\n",
    "\n",
    "starttime = datetime.now()\n",
    "\n",
    "### DEGREE CENTRALITY\n",
    "      \n",
    "#degr_c = nx.degree_centrality(LCC)\n",
    "\n",
    "#endtime = datetime.now()\n",
    "#elapsedtime = endtime - starttime\n",
    "\n",
    "#print(\"Elapsed time to degree centrality computation: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "### BETWEENNEESS CENTRALITY\n",
    "\n",
    "between = nx.betweenness_centrality(LCC)\n",
    "\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "\n",
    "print(\"Elapsed time to betweenness computation: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "### CLOSENESS CENTRALITY\n",
    "\n",
    "close = nx.closeness_centrality(LCC)\n",
    "\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "\n",
    "print(\"Elapsed time to closeness computation: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "### EIGENVECTOR CENTRALITY\n",
    "\n",
    "eigen = nx.eigenvector_centrality(LCC)\n",
    "\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "\n",
    "print(\"Elapsed time to eigenvector centrality computation: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "### DEGREE\n",
    "\n",
    "deg = LCC.degree()\n",
    "\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "\n",
    "print(\"Elapsed time to degree computation: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "# stop timing\n",
    "\n",
    "endtime = datetime.now()\n",
    "\n",
    "elapsedtime = endtime - starttime\n",
    "\n",
    "print(\"Total elapsed time: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "# saving results\n",
    "\n",
    "dataset_name_nx = \"Biogr_centr_res_NOW_networkx.csv\"\n",
    "\n",
    "print(\"Saving results to file\",dataset_name_nx,'\\n')\n",
    "\n",
    "# build a dataframe with all metrics\n",
    "df = pd.DataFrame({\n",
    "    \"Node\": list(LCC.nodes()),\n",
    "    \"degree\": [deg[n] for n in LCC],\n",
    "    \"betweenness\": [between[n] for n in LCC],\n",
    "    \"closeness\": [close[n] for n in LCC],\n",
    "    \"eigenvector_centr\": [eigen[n] for n in LCC],\n",
    "})\n",
    "\n",
    "# Show preview on console\n",
    "#print(df.head())\n",
    "\n",
    "# Sort by degree descending\n",
    "df_sorted = df.sort_values(by=\"degree\", ascending=False)\n",
    "\n",
    "# Print header once\n",
    "print(\"Node, degree, betweenness, closeness, eigenvector_centr\")\n",
    "\n",
    "# Print the first 50 rows\n",
    "for idx, row in df_sorted.head(50).iterrows():\n",
    "    print(f\"{row['Node']}, {row['degree']}, {row['betweenness']}, {row['closeness']}, {row['eigenvector_centr']}\")\n",
    "\n",
    "# Save to CSV\n",
    "\n",
    "df.to_csv(dataset_name_nx, index=False)\n",
    "\n",
    "print(\"\\nSaved to\",dataset_name_nx)\n",
    "\n",
    "\n",
    "\n",
    "#result_file = open('Biogrid_homo_1000_centr_results.txt', 'w')\n",
    "\n",
    "#print(\"Node, degree, betweenness, closeness, eigenvector_centr\")\n",
    "#print(\"Node, degree, betweenness, closeness, eigenvector_centr\", file = result_file)\n",
    "\n",
    "#for n in LCC:\n",
    "#    print((n, deg[n], between[n], close[n], eigen[n]))\n",
    "#    print((n, deg[n], between[n], close[n], eigen[n]), file = result_file)\n",
    "    \n",
    "#result_file.close()\n",
    "\n",
    "print()\n",
    "print(\"Game over\" +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Layouts\n",
    "\n",
    "# Define different layouts\n",
    "\n",
    "# Define different layouts\n",
    "layouts = {\n",
    "    \"Spring layout\": nx.spring_layout(LCC, seed=42),  # deterministic layout\n",
    "    \"Circular layout\": nx.circular_layout(LCC),\n",
    "    \"Kamada-Kawai layout\": nx.kamada_kawai_layout(LCC)\n",
    "}\n",
    "\n",
    "# Plot the graph using each layout\n",
    "plt.figure(figsize=(18, 6))  # wider figure for better readability\n",
    "\n",
    "for i, (name, layout) in enumerate(layouts.items(), 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    nx.draw(\n",
    "        LCC, \n",
    "        pos=layout, \n",
    "        with_labels=True,          # show node labels (protein names)\n",
    "        node_color='lightblue', \n",
    "        edge_color='gray', \n",
    "        node_size=600, \n",
    "        font_size=9,\n",
    "        font_weight='bold'\n",
    "    )\n",
    "    plt.title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layout for large graphs\n",
    "\n",
    "pos = nx.spring_layout(LCC, k=0.8, iterations=200, seed=42)  # k controls spacing\n",
    "\n",
    "plt.figure(figsize=(18, 18))  # large figure for clarity\n",
    "nx.draw(\n",
    "    LCC,\n",
    "    pos=pos,\n",
    "    with_labels=True,   \n",
    "    node_size=50,        # smaller nodes\n",
    "    node_color='skyblue',\n",
    "    edge_color='gray',\n",
    "    alpha=0.6            # slightly transparent edges\n",
    ")\n",
    "plt.title(\"LCC - Spring layout for larger graphs (~1000 nodes)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"current folder:\"\n",
    "pwd\n",
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "### iGraph centralities\n",
    "\n",
    "from igraph import Graph\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure NetworkX nodes have 'name' attribute\n",
    "for n in LCC.nodes():\n",
    "    LCC.nodes[n][\"name\"] = n\n",
    "\n",
    "# Manual conversion from NetworkX to igraph\n",
    "LCC_igraph = Graph()\n",
    "LCC_igraph.add_vertices(list(LCC.nodes()))  # add nodes with their names\n",
    "LCC_igraph.vs[\"name\"] = [n for n in LCC.nodes()]  # set the 'name' attribute\n",
    "\n",
    "# Add edges\n",
    "LCC_igraph.add_edges(list(LCC.edges()))\n",
    "\n",
    "starttime = datetime.now()\n",
    "\n",
    "### BETWEENNESS CENTRALITY (normalized)\n",
    "n = LCC_igraph.vcount()\n",
    "between = LCC_igraph.betweenness()\n",
    "between_norm = [b / ((n-1)*(n-2)/2) for b in between]\n",
    "print(\"Elapsed time to betweenness computation: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "### CLOSENESS CENTRALITY\n",
    "close = LCC_igraph.closeness()\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "print(\"Elapsed time to closeness computation: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "### EIGENVECTOR CENTRALITY\n",
    "#eigen = LCC_igraph.eigenvector_centrality()\n",
    "\n",
    "# eigenvector centrality in igraph, normalized to L2 = 1 like NetworkX\n",
    "eigen = LCC_igraph.eigenvector_centrality()\n",
    "# normalize to sum of squares = 1\n",
    "norm = sum([x**2 for x in eigen])**0.5\n",
    "eigen_l2 = [x / norm for x in eigen]\n",
    "# note that small differences still exist, after several decimals\n",
    "\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "print(\"Elapsed time to eigenvector centrality computation: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "### DEGREE\n",
    "deg = LCC_igraph.degree()\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "print(\"Elapsed time to degree computation: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "# stop timing\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "print(\"Total elapsed time: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "\n",
    "# saving results\n",
    "\n",
    "dataset_name_ig = \"Biogr_centr_res_NOW_igraph.csv\"\n",
    "\n",
    "print(\"Saving results to file\",dataset_name_ig,'\\n')\n",
    "\n",
    "# build a dataframe with all metrics\n",
    "df = pd.DataFrame({\n",
    "    \"Node\": list(LCC.nodes()),\n",
    "    \"degree\": [deg[LCC_igraph.vs.find(name=n).index] for n in LCC],\n",
    "    \"betweenness\": [between_norm[LCC_igraph.vs.find(name=n).index] for n in LCC],  # <--- normalized\n",
    "    \"closeness\": [close[LCC_igraph.vs.find(name=n).index] for n in LCC],\n",
    "    \"eigenvector_centr\": [eigen_l2[LCC_igraph.vs.find(name=n).index] for n in LCC],\n",
    "})\n",
    "\n",
    "\n",
    "# Sort by degree descending\n",
    "df_sorted = df.sort_values(by=\"degree\", ascending=False)\n",
    "\n",
    "# Print header once\n",
    "print(\"Node, degree, betweenness, closeness, eigenvector_centr\")\n",
    "\n",
    "# Print the first 50 rows\n",
    "for idx, row in df_sorted.head(50).iterrows():\n",
    "    print(f\"{row['Node']}, {row['degree']}, {row['betweenness']}, {row['closeness']}, {row['eigenvector_centr']}\")\n",
    "\n",
    "# Save to CSV\n",
    "\n",
    "df.to_csv(dataset_name_ig, index=False)\n",
    "\n",
    "print(\"\\nSaved to\",dataset_name_ig)\n",
    "\n",
    "\n",
    "\n",
    "############\n",
    "\n",
    "# saving results\n",
    "\n",
    "#dataset_name_ig = \"centr_res_igraph.csv\"\n",
    "\n",
    "#print(\"Saving results to file\",dataset_name_ig,'\\n')\n",
    "\n",
    "#with open(dataset_name_ig, 'w') as result_file_ig:\n",
    "#    header = \"Node, degree, betweenness, closeness, eigenvector_centr\"\n",
    "#    print(header)\n",
    "#    print(header, file=result_file_ig)\n",
    "\n",
    "#    for i, v in enumerate(LCC_igraph.vs):\n",
    "#        line = (v[\"name\"], deg[i], between_norm[i], close[i], eigen[i])\n",
    "#        print(line)\n",
    "#        print(line, file=result_file_ig)\n",
    "\n",
    "print()\n",
    "print(\"Game over for iGraph\" + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparing Networkx and iGraph speeds\n",
    "\n",
    "import networkx as nx\n",
    "from igraph import Graph\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Assume LCC is already a NetworkX graph\n",
    "nodes = list(LCC.nodes())\n",
    "\n",
    "### ------------------------------\n",
    "### NetworkX - normalized betweenness\n",
    "### ------------------------------\n",
    "start_nx = datetime.now()\n",
    "\n",
    "# Compute normalized betweenness centrality using NetworkX\n",
    "between_nx = nx.betweenness_centrality(LCC, normalized=True)\n",
    "\n",
    "end_nx = datetime.now()\n",
    "time_nx = (end_nx - start_nx).total_seconds()\n",
    "print(f\"NetworkX betweenness computation time: {time_nx:.4f} s\")\n",
    "\n",
    "### ------------------------------\n",
    "### iGraph - normalized betweenness\n",
    "### ------------------------------\n",
    "# Convert NetworkX graph to iGraph\n",
    "# Make sure node names are preserved\n",
    "\n",
    "for n in LCC.nodes():\n",
    "    LCC.nodes[n][\"name\"] = n\n",
    "\n",
    "LCC_igraph = Graph()\n",
    "LCC_igraph.add_vertices(list(LCC.nodes()))\n",
    "LCC_igraph.vs[\"name\"] = list(LCC.nodes())\n",
    "LCC_igraph.add_edges(list(LCC.edges()))\n",
    "\n",
    "start_ig = datetime.now()\n",
    "\n",
    "# Compute raw betweenness centrality in iGraph\n",
    "between_ig = LCC_igraph.betweenness()\n",
    "\n",
    "# Normalize betweenness as NetworkX does\n",
    "n = LCC_igraph.vcount()\n",
    "between_ig_norm = [b / ((n-1)*(n-2)/2) for b in between_ig]\n",
    "\n",
    "end_ig = datetime.now()\n",
    "time_ig = (end_ig - start_ig).total_seconds()\n",
    "print(f\"iGraph betweenness computation time: {time_ig:.4f} s\")\n",
    "\n",
    "### ------------------------------\n",
    "### Speed comparison\n",
    "### ------------------------------\n",
    "if time_ig > 0:\n",
    "    speed_ratio = time_nx / time_ig\n",
    "    print(f\"\\niGraph is approximately {speed_ratio:.2f} times faster than NetworkX\")\n",
    "else:\n",
    "    print(\"\\niGraph is practically instantaneous compared to NetworkX\")\n",
    "\n",
    "### ------------------------------\n",
    "### Show betweenness values for 10 random nodes\n",
    "### ------------------------------\n",
    "sample_nodes = random.sample(nodes, 10)\n",
    "print(\"\\nNode\\tNetworkX\\t iGraph\")\n",
    "for node in sample_nodes:\n",
    "    idx = LCC_igraph.vs.find(name=node).index\n",
    "    print(f\"{node}\\t{between_nx[node]:.6f}\\t{between_ig_norm[idx]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-crawford",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (netlab-venv)",
   "language": "python",
   "name": "netlab-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
