{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eaf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326bf78",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea079f5e",
   "metadata": {},
   "source": [
    "The files have already been preprocessed in R in `utils.R`\n",
    "\n",
    "Those were the parts for 1.1 and 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77996fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "biogrid_lcc = pd.read_csv(\"Files/Biogrid.txt\", sep='\\t')\n",
    "huri_lcc = pd.read_csv(\"Files/Huri.txt\", sep='\\t')\n",
    "string_lcc = pd.read_csv(\"Files/String.txt\", sep='\\t')\n",
    "reactome_lcc = pd.read_csv(\"Files/Reactome.txt\", sep='\\t')\n",
    "\n",
    "interactomes_lcc = [biogrid_lcc, huri_lcc, string_lcc, reactome_lcc]\n",
    "\n",
    "disease = pd.read_csv(\"Files/Cardiomyopathy.txt\").iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "biogrid_lcc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d8e4f",
   "metadata": {},
   "source": [
    "Build graph of interactome LCC size to find all the nodes and links (not just disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, name):\n",
    "    \"\"\"\n",
    "    Function th create graph\n",
    "\n",
    "    Params :\n",
    "    - df : the interactome\n",
    "    - name : name of the graph\n",
    "\n",
    "    Returns : instance of nxGraph\n",
    "    \"\"\"\n",
    "    # Create instance of graph\n",
    "    graph = nx.Graph(name = name)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(0,df.shape[0]):\n",
    "        # For each row, we add it as an edge\n",
    "        edges.append((df.iloc[i,0],df.iloc[i,1]))\n",
    "    \n",
    "    # Get list of *unique* genes among \"GeneA\" and \"GeneB\" columns\n",
    "    nodes = list(set(df[\"GeneA\"]) | set(df[\"GeneB\"]))\n",
    "\n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_edges_from(edges)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running for ~3 minutes...\")\n",
    "\n",
    "biogrid_lcc_graph = create_graph(biogrid_lcc, \"biogrid_interactions\")\n",
    "huri_lcc_graph = create_graph(huri_lcc, \"huri_interactions\")\n",
    "string_lcc_graph = create_graph(string_lcc, \"string_interactions\")\n",
    "reactome_lcc_graph = create_graph(reactome_lcc, \"reactome_interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ea344",
   "metadata": {},
   "source": [
    "### 1.3. Compute and characterize the disease LCC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb46c2",
   "metadata": {},
   "source": [
    "1.3.1 Check for the presence of disease genes in the LCCs of each interactome and identify the disease interactomes by getting the\n",
    "interactions among disease genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. We get the set of disease genes included in \"GeneA\" and \"GeneB\" for each interactome\n",
    "# 2. Then we create a new \"sub-interactome\" made only of disease genes interactions\n",
    "\n",
    "biogrid_filter = biogrid_lcc[\"GeneA\"].isin(disease) & biogrid_lcc[\"GeneB\"].isin(disease)\n",
    "biogrid_disease_interactions = biogrid_lcc[biogrid_filter].reset_index(drop=True)\n",
    "\n",
    "huri_filter = huri_lcc[\"GeneA\"].isin(disease) & huri_lcc[\"GeneB\"].isin(disease)\n",
    "huri_disease_interactions = huri_lcc[huri_filter].reset_index(drop=True)\n",
    "\n",
    "string_filter = string_lcc[\"GeneA\"].isin(disease) & string_lcc[\"GeneB\"].isin(disease)\n",
    "string_disease_interactions = string_lcc[string_filter].reset_index(drop=True)\n",
    "\n",
    "reactome_filter = reactome_lcc[\"GeneA\"].isin(disease) & reactome_lcc[\"GeneB\"].isin(disease)\n",
    "reactome_disease_interactions = reactome_lcc[reactome_filter].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a specific graph for the diseases interactions\n",
    "biogrid_disease_graph = create_graph(biogrid_disease_interactions, \"biogrid_disease_interactions\")\n",
    "huri_disease_graph = create_graph(huri_disease_interactions, \"huri_disease_interactions\")\n",
    "string_disease_graph = create_graph(string_disease_interactions, \"string_disease_interactions\")\n",
    "reactome_disease_graph = create_graph(reactome_disease_interactions, \"reactome_disease_interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248324d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing Biogrid disease graph\n",
    "nx.draw(biogrid_disease_graph, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc1199",
   "metadata": {},
   "source": [
    "1.3.2 Summarize the Interactome and GDA-related data as in table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_dict_loop = {\n",
    "    'biogrid': {},\n",
    "    'huri': {},\n",
    "    'string': {},\n",
    "    'reactome': {}\n",
    "}\n",
    "\n",
    "graphs = {\n",
    "    'biogrid': (biogrid_lcc_graph, biogrid_disease_graph),\n",
    "    'huri': (huri_lcc_graph, huri_disease_graph),\n",
    "    'string': (string_lcc_graph, string_disease_graph),\n",
    "    'reactome': (reactome_lcc_graph, reactome_disease_graph)\n",
    "}\n",
    "\n",
    "for db_name, (lcc_graph, disease_graph) in graphs.items():\n",
    "    # 1. Interactome size\n",
    "    table1_dict_loop[db_name][\"nodes_edges\"] = (lcc_graph.number_of_nodes(), lcc_graph.number_of_edges())\n",
    "    \n",
    "    # 2. Number of disease genes\n",
    "    table1_dict_loop[db_name][\"nbr_disease_genes\"] = disease_graph.number_of_nodes()\n",
    "    \n",
    "    # 3. Percentage of disease genes\n",
    "    table1_dict_loop[db_name][\"percent\"] = round((disease_graph.number_of_nodes() / lcc_graph.number_of_nodes()) * 100, 2)\n",
    "    \n",
    "    # 4. Disease LCC size\n",
    "    largest_cc = max(nx.connected_components(disease_graph), key=len)\n",
    "    table1_dict_loop[db_name][\"disease_lcc_size\"] = len(largest_cc)\n",
    "\n",
    "table1_dict_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cddd33",
   "metadata": {},
   "source": [
    "1.3.3 Compute those metrics on the largest disease LCC :\n",
    "- Node degree\n",
    "- Betweenness centrality\n",
    "- Eigenvector centrality\n",
    "- Closeness centrality\n",
    "- ratio Betweenness/Node degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a15f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest disease LCC across all databases\n",
    "largest_LCC_size = max(table1_dict_loop[db][\"disease_lcc_size\"] for db in table1_dict_loop)\n",
    "largest_LCC_name = max(table1_dict_loop, key=lambda db: table1_dict_loop[db][\"disease_lcc_size\"])\n",
    "\n",
    "# Sanity check - verify all LCC sizes are <= the largest\n",
    "for db_name in table1_dict_loop:\n",
    "    assert table1_dict_loop[db_name][\"disease_lcc_size\"] <= largest_LCC_size\n",
    "\n",
    "print(f\"Largest disease LCC: {largest_LCC_name} with {largest_LCC_size} nodes\")\n",
    "\n",
    "disease_graph = graphs[largest_LCC_name][1]\n",
    "largest_cc = max(nx.connected_components(disease_graph), key=len)\n",
    "largest_LCC = disease_graph.subgraph(largest_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37901987",
   "metadata": {},
   "source": [
    "We note that the largest disease LCC is from the String interactome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_time_infos(starttime, metric):\n",
    "    endtime = datetime.now()\n",
    "    elapsedtime = endtime - starttime\n",
    "    print(f\"Elapsed time to {metric} computation: \" + str(elapsedtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fe9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "\n",
    "# 1. Node degree\n",
    "degrees = largest_LCC.degree()\n",
    "call_time_infos(starttime, \"degree\")\n",
    "\n",
    "# 2. Betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(largest_LCC)\n",
    "call_time_infos(starttime, \"betweenness\")\n",
    "\n",
    "# 3. Closeness centrality\n",
    "closeness = nx.closeness_centrality(largest_LCC)\n",
    "call_time_infos(starttime, \"closeness\")\n",
    "\n",
    "# 4. Eigenvector centrality\n",
    "eigen = nx.eigenvector_centrality(largest_LCC)\n",
    "call_time_infos(starttime, \"eigenvector centrality\")\n",
    "\n",
    "# Stop timing\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "print(\"Total elapsed time: \" + str(elapsedtime) + '\\n')\n",
    "\n",
    "# build a dataframe with all metrics\n",
    "metrics_largest_LCC = pd.DataFrame({\n",
    "    \"Node\": list(largest_LCC.nodes()),\n",
    "    \"degree\": [degrees[n] for n in largest_LCC],\n",
    "    \"betweenness\": [betweenness[n] for n in largest_LCC],\n",
    "    \"closeness\": [closeness[n] for n in largest_LCC],\n",
    "    \"eigenvector_centrality\": [eigen[n] for n in largest_LCC],\n",
    "    \"ratio_betweenness_degree\": [betweenness[n]/degrees[n] for n in largest_LCC]\n",
    "})\n",
    "\n",
    "# Sort by degree descending\n",
    "metrics_largest_LCC = metrics_largest_LCC.sort_values(by=\"degree\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "metrics_largest_LCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf122ac",
   "metadata": {},
   "source": [
    "1.3.4 Report in a table the above network measures of the first 20 disease genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print header once\n",
    "top20_disease_genes_degrees = metrics_largest_LCC.iloc[0:20,:]\n",
    "top20_disease_genes_degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c6705",
   "metadata": {},
   "source": [
    "1.3.5 Represent node degree and node betweenness in a scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecaa545",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_values = [d for d in metrics_largest_LCC['degree'].tolist()]\n",
    "betweenness_values = [d for d in metrics_largest_LCC['betweenness'].tolist()]\n",
    "\n",
    "# TODO : See if this scatter is relevant\n",
    "plt.scatter(range(len(degree_values)), degree_values, alpha=0.6)\n",
    "plt.xlabel(\"Nodes indexes (ranked as those on the right have smaller degree)\")\n",
    "plt.ylabel(\"Degree\")\n",
    "plt.title(\"Node degree scatter plot (disease LCC)\")\n",
    "plt.show()\n",
    "\n",
    "# TODO : See if this scatter is relevant\n",
    "plt.scatter(range(len(betweenness_values)), betweenness_values, alpha=0.6)\n",
    "plt.xlabel(\"Nodes indexes (ranked as those on the right have smaller degree)\")\n",
    "plt.ylabel(\"Betweenness centrality\")\n",
    "plt.title(\"Node betweenness centrality scatter plot (disease LCC)\")\n",
    "plt.show()\n",
    "\n",
    "# TODO : check (ask the prof ?) if this scatter is the one he wants ?\n",
    "plt.scatter(degree_values, betweenness_values, alpha=0.6)\n",
    "for node in metrics_largest_LCC['Node']:\n",
    "    # We add gene symbols on the graph for significative genes (in terms of betweenness)\n",
    "    if betweenness[node] > 0.01:\n",
    "        plt.text(degrees[node], betweenness[node], node, fontsize=8)\n",
    "plt.xlabel(\"Node degree\")\n",
    "plt.ylabel(\"Betweenness centrality\")\n",
    "plt.title(\"Degree VS Betweenness centrality scatter plot (disease LCC)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797952b",
   "metadata": {},
   "source": [
    "As we can see, there is a correlation between node degree and betweenness centrality. The more connected is the node, the geater is the betweenness centrality.\n",
    "\n",
    "Betweenness centrality is the measure of how much a node is a \"bridge/bottleneck\" in other node pathways.\n",
    "\n",
    "The most interesting nodes would be those with low degree and high betweenness ! Unfortunately here this scenario is doesn't appear. Maybe for `VCP` gene but not enough evidence..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d237dd",
   "metadata": {},
   "source": [
    "## 2. Comparative analysis of disease genes identification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e19e22",
   "metadata": {},
   "source": [
    "### 2.1 Use algorithms to infer and then validate putative disease genes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24cee83",
   "metadata": {},
   "source": [
    "2.1.1 DIAMOnD, default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acb338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With n=200, running for ~1 minute\n",
    "!python DIAMOnD.py Files/Biogrid.txt Files/Cardiomyopathy.txt 200 Files/diamond_output_200.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3484bc",
   "metadata": {},
   "source": [
    "2.1.2 Diffusion-based, diffusion times: t=[0.1, 1.0, 2.0, 5.0, 10.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7335de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With t=0.1, running for ~4 seconds\n",
    "!python HeatDiffusion.py Files/Biogrid.txt Files/Cardiomyopathy.txt 200 0.1 Files/heat_diffusion_output_0.1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d68ecc",
   "metadata": {},
   "source": [
    "2.1.3 OPTIONAL : Run Proconsul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d270a",
   "metadata": {},
   "source": [
    "### 2.2 Computational validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2300a2",
   "metadata": {},
   "source": [
    "2.2.1 For each interactome: perform a 5-fold cross validation in the following way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f8966",
   "metadata": {},
   "source": [
    "Split the disease genes set $S_0$ into 5 subsets.\n",
    "\n",
    "For each fold, select one subset as probe set $S_P$ and the remaining four subsets as training set $S_T$.\n",
    "\n",
    "For every fold, run both algorithms on all four interactomes (8 combinations, 40 runs total) using the training $S_T$ sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee548948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algo(algo, network, train_set, output_file, fold_idx, n, t=None):\n",
    "    \"\"\"\n",
    "    Run DIAMOnD algorithm on the selected fold, save prediction.\n",
    "\n",
    "    Params :\n",
    "    - algo : algorithm to run [\"diamond\" | \"heatdiffusion\"]\n",
    "    - network : interactome\n",
    "    - train_set : subset of disease gene\n",
    "    - output_file : name/path for the saved file\n",
    "    - fold_idx : index of the current fold\n",
    "    - n : number of disease genes to predict\n",
    "    - t : diffusion times (for heatdiffusion)\n",
    "    \"\"\"\n",
    "\n",
    "    if algo==\"diamond\":\n",
    "        print(\"Running Diamond for ~1 minute\")\n",
    "        diamond_script = \"DIAMOnD.py\"\n",
    "        cmd = ['python', diamond_script, network, train_set, str(n), output_file]\n",
    "    elif algo==\"heatdiffusion\":\n",
    "        print(\"Running heat diffusion for few seconds\")\n",
    "        diamond_script = \"HeatDiffusion.py\"\n",
    "        assert(t!=None)\n",
    "        cmd = ['python', diamond_script, network, train_set, str(n), str(t), output_file]\n",
    "    else:\n",
    "        raise AttributeError(algo+\" not found. Shoud be 'diamond' or 'heatdiffusion'.\")\n",
    "\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "        print(f\"✓ Fold {fold_idx} completed successfully\")\n",
    "        print(result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ Error in Fold {fold_idx}\")\n",
    "        print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edf4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def run_algo_on_one_interactome(interactome, disease, K, n, try_multiple_t=False):\n",
    "    \"\"\"\n",
    "    Create K-folds and run algorithms for the specified interactome for the K folds.\n",
    "    \n",
    "    Parameters :\n",
    "    - interactome : path to the interactome file\n",
    "    - disease : set of disease genes\n",
    "    - K : number of folds\n",
    "    - n : number of genes to predict\n",
    "    - try_multiple_t : if testing different values of diffusion time\n",
    "    \"\"\"\n",
    "\n",
    "    # Load folds\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "    # Iterate on each fold (train, test)\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(disease), 1):\n",
    "        print(f\"Processing Fold {fold_idx}\")\n",
    "        \n",
    "        # Get the disease genes from the current train and test fold\n",
    "        train_genes = [disease[i] for i in train_idx]\n",
    "        test_genes = [disease[i] for i in test_idx]\n",
    "        train_file = f'Files/Folds/train_fold_{fold_idx}.txt'\n",
    "        test_file = f'Files/Folds/test_fold_{fold_idx}.txt'\n",
    "        \n",
    "        # Save train (for algorithms)\n",
    "        with open(train_file, 'w') as f:\n",
    "            f.write('\\n'.join(train_genes))\n",
    "        \n",
    "        # Save test (to evaluate)\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write('\\n'.join(test_genes))\n",
    "\n",
    "        # Diamond\n",
    "        run_algo(\"diamond\", interactome, train_file, f'Files/Folds/{interactome.removesuffix(\".txt\").removeprefix(\"Files/\")}_diamond_results_fold_{fold_idx}.txt', fold_idx, n)\n",
    "\n",
    "        # Heat diffusion\n",
    "        if try_multiple_t:\n",
    "            t = [0.1, 1.0, 2.0, 5.0, 10.0]\n",
    "            for time in t:\n",
    "                output_file = f'Files/Folds/{interactome.removesuffix(\".txt\").removeprefix(\"Files/\")}_heatdiffusion_results_fold_{fold_idx}_{time}.txt'\n",
    "                run_algo(\"heatdiffusion\", interactome, train_file, output_file, fold_idx, n, t=time)\n",
    "        else:\n",
    "            t = 1.0 # time diffusion selected after comparing in 'try_multiple_t' mode\n",
    "            output_file = f'Files/Folds/{interactome.removesuffix(\".txt\").removeprefix(\"Files/\")}_heatdiffusion_results_fold_{fold_idx}_{t}.txt'\n",
    "            run_algo(\"heatdiffusion\", interactome, train_file, output_file, fold_idx, n, t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running for ~32 minutes...\")\n",
    "\n",
    "run_algo_on_one_interactome(\"Files/Biogrid.txt\", disease, K=5, n=200, try_multiple_t=False)\n",
    "run_algo_on_one_interactome(\"Files/Huri.txt\", disease, K=5, n=200, try_multiple_t=False)\n",
    "run_algo_on_one_interactome(\"Files/String.txt\", disease, K=5, n=200, try_multiple_t=False)\n",
    "run_algo_on_one_interactome(\"Files/Reactome.txt\", disease, K=5, n=200, try_multiple_t=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b19f2",
   "metadata": {},
   "source": [
    "Evaluate each run by checking how many genes from the probe set $S_P$ appear in the algorithm’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cadf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(prediction_file, true_file, top_X=None):\n",
    "    \"\"\"\n",
    "    Evaluate the algorithm by comparing the expected genes from test_set with the set of genes in the result_file\n",
    "    \n",
    "    Parameters :\n",
    "    - prediction_file : path to the file containing the predictions of disease genes obtained after running the algorithms\n",
    "    - true_genes_file : set of genes expected in the result_file\n",
    "    - top_X : to evalute only on top X genes in prediction_file\n",
    "\n",
    "    Returns :\n",
    "    - Dictionnary with all the metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates a set with the real disease genes\n",
    "    true_genes = {line.strip() for line in open(true_file)}\n",
    "\n",
    "    # If top_X not provided, we analyse the whole file (top_X = nb lines)\n",
    "    if top_X==None:\n",
    "        with open(prediction_file) as f:\n",
    "            top_X = len(f.readlines())\n",
    "\n",
    "    # Creates a set with the predicted genes\n",
    "    predicted_genes = {\n",
    "        line.strip().split('\\t')[1]\n",
    "        for i, line in enumerate(open(prediction_file))\n",
    "        if not line.startswith('#') and i < top_X # We skip the first line (with \"#\") and stop until top_X\n",
    "    }\n",
    "\n",
    "    # Compute true positive, false positive and false negative for the metrics\n",
    "    TP = len(predicted_genes & true_genes)\n",
    "    FP = len(predicted_genes - true_genes)\n",
    "    FN = len(true_genes - predicted_genes)\n",
    "\n",
    "    # Compute required metrics\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_prediction(top_X=None):\n",
    "    interactomes = [\"Biogrid\", \"Huri\", \"Reactome\", \"String\"]\n",
    "    results_data = []\n",
    "\n",
    "    # Iterate on each fold\n",
    "    for fold in range(1, 6):\n",
    "        # Iterate on each interactome\n",
    "        for interactome in interactomes:\n",
    "            for X in (top_X if top_X is not None else [None]):\n",
    "                # Diamond\n",
    "                results_diamond = evaluate_predictions(\n",
    "                    prediction_file=f'Files/Folds/{interactome}_diamond_results_fold_{fold}.txt',\n",
    "                    true_file=f'Files/Folds/test_fold_{fold}.txt',\n",
    "                    top_X=X\n",
    "                )\n",
    "                results_data.append({\n",
    "                    'Top_X': X,\n",
    "                    'Fold': fold,\n",
    "                    'Interactome': interactome,\n",
    "                    'Algorithm': 'Diamond',\n",
    "                    'Precision': results_diamond['precision'],\n",
    "                    'Recall': results_diamond['recall'],\n",
    "                    'F1': results_diamond['f1'],\n",
    "                    'TP': results_diamond['TP'],\n",
    "                    'FN': results_diamond['FN']\n",
    "                })\n",
    "                \n",
    "                # Heat Diffusion\n",
    "                results_heat = evaluate_predictions(\n",
    "                    prediction_file=f'Files/Folds/{interactome}_heatdiffusion_results_fold_{fold}_1.0.txt',\n",
    "                    true_file=f'Files/Folds/test_fold_{fold}.txt',\n",
    "                    top_X=X\n",
    "                )\n",
    "                results_data.append({\n",
    "                        'Top_X': X,\n",
    "                        'Fold': fold,\n",
    "                        'Interactome': interactome,\n",
    "                        'Algorithm': 'HeatDiffusion',\n",
    "                        'Precision': results_heat['precision'],\n",
    "                        'Recall': results_heat['recall'],\n",
    "                        'F1': results_heat['f1'],\n",
    "                        'TP': results_heat['TP'],\n",
    "                        'FN': results_heat['FN']\n",
    "                    })\n",
    "                \n",
    "    return results_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd995696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_metrics(df):\n",
    "    # Comparing algorithms and interactomes\n",
    "    algo_comparison = df.groupby('Algorithm')['Recall'].agg(['mean', 'std', 'min', 'max'])\n",
    "    algo_comparison['mean_pct'] = algo_comparison['mean'] * 100\n",
    "    algo_comparison['std_pct'] = algo_comparison['std'] * 100\n",
    "\n",
    "    interactome_comparison = df.groupby('Interactome')['Recall'].agg(['mean', 'std', 'min', 'max'])\n",
    "    interactome_comparison['mean_pct'] = interactome_comparison['mean'] * 100\n",
    "    interactome_comparison['std_pct'] = interactome_comparison['std'] * 100\n",
    "    interactome_comparison = interactome_comparison.sort_values('mean_pct', ascending=False)\n",
    "\n",
    "    # ============================================================================\n",
    "    # Executive summary\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXECUTIVE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    best_algo = algo_comparison['mean_pct'].idxmax()\n",
    "    best_algo_recall = algo_comparison.loc[best_algo, 'mean_pct']\n",
    "\n",
    "    best_interactome_overall = interactome_comparison['mean_pct'].idxmax()\n",
    "    best_interactome_recall = interactome_comparison.loc[best_interactome_overall, 'mean_pct']\n",
    "\n",
    "    # Best combinaison\n",
    "    best_combo = df.groupby(['Algorithm', 'Interactome'])['Recall'].mean().idxmax()\n",
    "    best_combo_recall = df.groupby(['Algorithm', 'Interactome'])['Recall'].mean().max() * 100\n",
    "\n",
    "    print(f\"\"\"\n",
    "    1. Best Algorithm:     {best_algo} ({best_algo_recall:.2f}% average recall)\n",
    "    2. Best Interactome:   {best_interactome_overall} ({best_interactome_recall:.2f}% average recall)\n",
    "    3. Best Combination:   {best_combo[0]} + {best_combo[1]} ({best_combo_recall:.2f}% recall)\n",
    "    \"\"\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # Visualizing\n",
    "    # ============================================================================\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
    "\n",
    "    # Plot 1: Barplot - Algorithmes\n",
    "    algo_means = df.groupby('Algorithm')['Recall'].mean() * 100\n",
    "    axes[0].bar(algo_means.index, algo_means.values, color=['blue', 'red'])\n",
    "    axes[0].set_ylabel('Average Recall (%)', fontsize=12)\n",
    "    axes[0].set_title('Algorithm Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(algo_means.values):\n",
    "        axes[0].text(i, v + 1, f'{v:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "    # Plot 2: Barplot - Interactomes\n",
    "    interactome_means = df.groupby('Interactome')['Recall'].mean().sort_values(ascending=False) * 100\n",
    "    axes[1].bar(interactome_means.index, interactome_means.values, \n",
    "                color=['green', 'orange', 'purple', 'blue'])\n",
    "    axes[1].set_ylabel('Average Recall (%)', fontsize=12)\n",
    "    axes[1].set_title('Interactome Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(interactome_means.values):\n",
    "        axes[1].text(i, v + 1, f'{v:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "    # Plot 3: Heatmap - Algorithm × Interactome\n",
    "    heatmap_data = df.pivot_table(values='Recall', index='Interactome', \n",
    "                                columns='Algorithm', aggfunc='mean') * 100\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "                ax=axes[2], cbar_kws={'label': 'Recall (%)'})\n",
    "    axes[2].set_title('Recall Heatmap: Algorithm × Interactome', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efe729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "\n",
    "results_data = iterate_prediction(top_X=None)\n",
    "df = pd.DataFrame(results_data)\n",
    "\n",
    "# Create a dataframe specific to display only what's ask in this questions\n",
    "display_df = df.assign(**{\n",
    "        'Disease genes found': df['TP'],\n",
    "        'Total disease genes': df['TP'] + df['FN']\n",
    "    })[['Fold','Interactome','Algorithm','Disease genes found','Total disease genes']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED RESULTS (all folds)\")\n",
    "print(\"=\"*80)\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a96a0",
   "metadata": {},
   "source": [
    "2.2.2 Compute the following performance metrics for all the combinations algorithm/interactome:\n",
    "- precision (average SD)\n",
    "- recall (average SD)\n",
    "- F1-score (average SD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e72a07",
   "metadata": {},
   "source": [
    "Precision : $$\\frac{\\text{True positive}}{\\text{True Positive + False Positive}}$$\n",
    "\n",
    "Recall : $$\\frac{\\text{True positive}}{\\text{True Positive + False Negative}}$$\n",
    "\n",
    "F1-Score : $$ 2 * \\frac{\\text{precision} * \\text{recall}}{\\text{precision + recall}}$$\n",
    "\n",
    "- **True positive** corresponds to predicted real disease genes\n",
    "- **False positive** corresponds to predicted not disease genes\n",
    "- **False negative** corresponds to the disease genes not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summary = (\n",
    "    df\n",
    "    .groupby(['Algorithm', 'Interactome'])\n",
    "    .agg({\n",
    "        'Precision': ['mean', 'std'],\n",
    "        'Recall': ['mean', 'std'],\n",
    "        'F1': ['mean', 'std']\n",
    "    })\n",
    ")\n",
    "\n",
    "# Display\n",
    "metrics_summary.columns = [\n",
    "    f\"{metric}_{stat}\"\n",
    "    for metric, stat in metrics_summary.columns\n",
    "]\n",
    "\n",
    "metrics_summary = metrics_summary.reset_index()\n",
    "\n",
    "metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d89f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary_metrics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e6c947",
   "metadata": {},
   "source": [
    "In conclusion, the best combinasion is using String interactome with the heat diffusion algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832d44b",
   "metadata": {},
   "source": [
    "2.2.3 Provide the performance measures selecting the top 50 positions and the top X positions where $X$ = $\\frac{n}{8}$, $\\frac{n}{4}$, $\\frac{n}{2}$, $n$, with n=number of known GDAs (i.e., number of disease’s seed genes)\n",
    "\n",
    "In our case we have n=373 disease genes in $S_0$. So we'll keep the following top_X :\n",
    "- $X = 50$\n",
    "- $X = \\frac{n}{8} = 47$\n",
    "- $X = \\frac{n}{4} = 93$\n",
    "- $X = \\frac{n}{2} = 186$\n",
    "- $X = n = 373$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245eac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(disease) # n = number of total disease gene set (S_0)\n",
    "top_X = [50, round(n/8), round(n/4), round(n/2), n]\n",
    "\n",
    "results_data_topX = iterate_prediction(top_X=top_X)\n",
    "# Create dataframe\n",
    "df_topX = pd.DataFrame(results_data_topX)\n",
    "\n",
    "metrics_summary_topX = (\n",
    "    df_topX\n",
    "    .groupby(['Top_X', 'Algorithm', 'Interactome'])\n",
    "    .agg({\n",
    "        'Precision': ['mean', 'std'],\n",
    "        'Recall': ['mean', 'std'],\n",
    "        'F1': ['mean', 'std']\n",
    "    })\n",
    ")\n",
    "\n",
    "# Display\n",
    "metrics_summary_topX.columns = [\n",
    "    f\"{metric}_{stat}\"\n",
    "    for metric, stat in metrics_summary_topX.columns\n",
    "]\n",
    "\n",
    "metrics_summary_topX = metrics_summary_topX.reset_index()\n",
    "\n",
    "metrics_summary_topX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary_metrics(df_topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb3d9c",
   "metadata": {},
   "source": [
    "In conclusion, even after evaluating on several top_X predicted genes, we still have the best combination : String interactome with heat diffusion algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e493573",
   "metadata": {},
   "source": [
    "## 3. Putative disease gene identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754c0ed",
   "metadata": {},
   "source": [
    "### 3.1.  According to the performance metrics obtained in the validation phase at point 2: \n",
    "\n",
    "3.1.1 select the best performing algorithm and the best interactome and apply the \n",
    "process  to  predict  new  putative  disease  genes  using  all known GDAs as \n",
    "seed genes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ecb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def run_heatdiffusion_all_seeds(interactome_file, disease_genes, output_file, t=1.0):\n",
    "    \"\"\"\n",
    "    Run HeatDiffusion algorithm using ALL disease genes as seeds.\n",
    "    Uses the existing HeatDiffusion.py script.\n",
    "    \n",
    "    Parameters:\n",
    "    - interactome_file: path to interactome file (e.g., 'Files/String.txt')\n",
    "    - disease_genes: list of all disease gene names\n",
    "    - output_file: where to save the results\n",
    "    - t: heat diffusion time parameter (default=1.0)\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"Running HeatDiffusion with ALL Seeds\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Interactome: {interactome_file}\")\n",
    "    print(f\"Number of seed genes: {len(disease_genes)}\")\n",
    "    print(f\"Time parameter t: {t}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    \n",
    "    # Create temporary seeds file with ALL disease genes\n",
    "    seeds_file = \"Files/all_seeds_temp.txt\"\n",
    "    with open(seeds_file, 'w') as f:\n",
    "        f.write('\\n'.join(disease_genes))\n",
    "    \n",
    "    print(f\"\\n✓ Created seeds file with {len(disease_genes)} genes\")\n",
    "    \n",
    "    # Run HeatDiffusion using existing script\n",
    "    # Note: n should be large enough to get all genes in the network\n",
    "    # We'll use a large number (e.g., 20000) to ensure we get all predictions\n",
    "    n = 20000  # Large number to get all predictions\n",
    "    \n",
    "    heatdiffusion_script = \"HeatDiffusion.py\"\n",
    "    cmd = ['python', heatdiffusion_script, interactome_file, seeds_file, str(n), str(t), output_file]\n",
    "    \n",
    "    print(f\"\\nRunning: {' '.join(cmd)}\")\n",
    "    print(\"(This may take a few minutes...)\\n\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=600)\n",
    "        print(\"✓ HeatDiffusion completed successfully!\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Verify output file exists\n",
    "        if os.path.exists(output_file):\n",
    "            # Load and display results\n",
    "            df = pd.read_csv(output_file, sep='\\t', header=None, names=['Gene', 'Score'])\n",
    "            print(f\"✓ Generated predictions for {len(df)} genes\")\n",
    "            print(f\"\\nTop 20 predicted genes:\")\n",
    "            print(df.head(20))\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"✗ Output file not created: {output_file}\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ HeatDiffusion failed with error:\")\n",
    "        print(e.stderr)\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n✗ HeatDiffusion.py script not found!\")\n",
    "        print(\"Make sure HeatDiffusion.py is in the current directory\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error running HeatDiffusion: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running HeatDiffusion with {len(disease)} seed genes...\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "output_file = \"Files/String_heatdiffusion_all_seeds.txt\"\n",
    "\n",
    "heatdiffusion_results = run_heatdiffusion_all_seeds(\n",
    "    interactome_file=\"Files/String.txt\",\n",
    "    disease_genes=disease,\n",
    "    output_file=output_file,\n",
    "    t=1.0\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"\\nTime elapsed: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_putative_genes(predictions_df, known_disease_genes, top_n=100):\n",
    "    \"\"\"\n",
    "    Extract top N putative disease genes from predictions.\n",
    "    Excludes known disease genes.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions_df: DataFrame with 'Gene' and 'Score' columns\n",
    "    - known_disease_genes: list of known disease genes to exclude\n",
    "    - top_n: number of putative genes to return\n",
    "    \n",
    "    Returns:\n",
    "    - list of putative gene names\n",
    "    - DataFrame with putative genes and scores\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Extracting Top {top_n} Putative Disease Genes\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Filter out known disease genes\n",
    "    putative_df = predictions_df[~predictions_df['Gene'].isin(known_disease_genes)].copy()\n",
    "    \n",
    "    print(f\"Total predictions: {len(predictions_df)}\")\n",
    "    print(f\"After removing {len(known_disease_genes)} known disease genes: {len(putative_df)}\")\n",
    "    \n",
    "    # Get top N\n",
    "    top_putative = putative_df.head(top_n).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nTop 10 putative disease genes:\")\n",
    "    print(top_putative.head(10))\n",
    "    \n",
    "    return top_putative['Gene'].tolist(), top_putative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd50053",
   "metadata": {},
   "outputs": [],
   "source": [
    "putative_genes, putative_df = get_putative_genes(\n",
    "    heatdiffusion_results,\n",
    "    disease,\n",
    "    top_n=100\n",
    ")\n",
    "\n",
    "putative_df.to_csv(\"Files/putative_genes_top100.csv\", index=False)\n",
    "print(f\"\\n✓ Saved top 100 putative genes to Files/putative_genes_top100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397bfaa",
   "metadata": {},
   "source": [
    "3.2.1: Enrichment Analysis for Putative Disease Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def enrichr_analysis(gene_list, gene_set_libraries, description=\"\"):\n",
    "    \"\"\"\n",
    "    Perform enrichment analysis using EnrichR API.\n",
    "    \n",
    "    Parameters:\n",
    "    - gene_list: list of gene symbols\n",
    "    - gene_set_libraries: list of library names\n",
    "    - description: description for the gene list\n",
    "    \n",
    "    Returns:\n",
    "    - dictionary with results for each library\n",
    "    \"\"\"\n",
    "    ENRICHR_URL = 'https://maayanlab.cloud/Enrichr'\n",
    "    \n",
    "    # Submit gene list\n",
    "    genes_str = '\\n'.join(gene_list)\n",
    "    payload = {\n",
    "        'list': (None, genes_str),\n",
    "        'description': (None, description)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EnrichR Analysis: {description}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Submitting {len(gene_list)} genes...\")\n",
    "    \n",
    "    response = requests.post(f'{ENRICHR_URL}/addList', files=payload)\n",
    "    \n",
    "    if not response.ok:\n",
    "        raise Exception(f'Error analyzing gene list: {response.text}')\n",
    "    \n",
    "    data = json.loads(response.text)\n",
    "    user_list_id = data['userListId']\n",
    "    print(f\"✓ List ID: {user_list_id}\")\n",
    "    \n",
    "    # Get enrichment results for each library\n",
    "    all_results = {}\n",
    "    \n",
    "    for library in gene_set_libraries:\n",
    "        print(f\"\\nQuerying {library}...\")\n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "        \n",
    "        response = requests.get(\n",
    "            f'{ENRICHR_URL}/enrich',\n",
    "            params={\n",
    "                'userListId': user_list_id,\n",
    "                'backgroundType': library\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if not response.ok:\n",
    "            print(f\"⚠ Warning: Could not get results for {library}\")\n",
    "            continue\n",
    "        \n",
    "        data = json.loads(response.text)\n",
    "        \n",
    "        if library not in data or len(data[library]) == 0:\n",
    "            print(f\"  No results for {library}\")\n",
    "            all_results[library] = pd.DataFrame()\n",
    "            continue\n",
    "        \n",
    "        # Parse results\n",
    "        results = data[library]\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(results, columns=[\n",
    "            'Rank', 'Term', 'P-value', 'Z-score', 'Combined Score',\n",
    "            'Overlapping Genes', 'Adjusted P-value', 'Old P-value', 'Old Adjusted P-value'\n",
    "        ])\n",
    "        \n",
    "        # Filter for significant results (adjusted p-value < 0.05)\n",
    "        df_sig = df[df['Adjusted P-value'] < 0.05].copy()\n",
    "        \n",
    "        print(f\"  ✓ Found {len(df_sig)} significant terms (adj p-value < 0.05)\")\n",
    "        \n",
    "        all_results[library] = df_sig\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64030bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries = [\n",
    "    'GO_Biological_Process_2023',\n",
    "    'GO_Molecular_Function_2023',\n",
    "    'GO_Cellular_Component_2023',\n",
    "    'Reactome_2022',\n",
    "    'KEGG_2021_Human'\n",
    "]\n",
    "\n",
    "print(\"Gene Set Libraries:\")\n",
    "for i, lib in enumerate(libraries, 1):\n",
    "    print(f\"  {i}. {lib}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6998ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "putative_enrichment = enrichr_analysis(\n",
    "    putative_genes,\n",
    "    libraries,\n",
    "    description=\"Cardiomyopathy_Putative_Genes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d26a39",
   "metadata": {},
   "source": [
    "3.2.2: Enrichment Analysis for Original Disease Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ca007",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_enrichment = enrichr_analysis(\n",
    "    disease,\n",
    "    libraries,\n",
    "    description=\"Cardiomyopathy_Original_Genes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012209f2",
   "metadata": {},
   "source": [
    "3.2.3: Compare Enriched Functions (Overlap Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e865f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_enrichment_results(original_results, putative_results, library_name):\n",
    "    \"\"\"\n",
    "    Compare enrichment results between original and putative disease genes.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with overlap analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Comparing Enrichment: {library_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if library_name not in original_results or library_name not in putative_results:\n",
    "        print(f\"⚠ Results not available for {library_name}\")\n",
    "        return None\n",
    "    \n",
    "    orig_df = original_results[library_name]\n",
    "    put_df = putative_results[library_name]\n",
    "    \n",
    "    if orig_df.empty or put_df.empty:\n",
    "        print(f\"⚠ No significant results in one or both gene sets\")\n",
    "        return None\n",
    "    \n",
    "    # Get term names\n",
    "    orig_terms = set(orig_df['Term'].tolist())\n",
    "    put_terms = set(put_df['Term'].tolist())\n",
    "    \n",
    "    # Find overlap\n",
    "    overlap_terms = orig_terms & put_terms\n",
    "    orig_only = orig_terms - put_terms\n",
    "    put_only = put_terms - orig_terms\n",
    "    \n",
    "    print(f\"\\nOriginal disease genes: {len(orig_terms)} significant terms\")\n",
    "    print(f\"Putative disease genes: {len(put_terms)} significant terms\")\n",
    "    print(f\"Overlapping terms: {len(overlap_terms)}\")\n",
    "    print(f\"Original only: {len(orig_only)}\")\n",
    "    print(f\"Putative only: {len(put_only)}\")\n",
    "    \n",
    "    if len(overlap_terms) > 0:\n",
    "        # Calculate overlap percentage\n",
    "        overlap_pct = (len(overlap_terms) / min(len(orig_terms), len(put_terms))) * 100\n",
    "        print(f\"Overlap percentage: {overlap_pct:.1f}%\")\n",
    "        \n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(\"Top 10 Overlapping Enriched Terms:\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        overlap_list = []\n",
    "        for term in overlap_terms:\n",
    "            orig_row = orig_df[orig_df['Term'] == term].iloc[0]\n",
    "            put_row = put_df[put_df['Term'] == term].iloc[0]\n",
    "            \n",
    "            overlap_list.append({\n",
    "                'Term': term,\n",
    "                'Original_P_adj': orig_row['Adjusted P-value'],\n",
    "                'Putative_P_adj': put_row['Adjusted P-value'],\n",
    "                'Original_Combined_Score': orig_row['Combined Score'],\n",
    "                'Putative_Combined_Score': put_row['Combined Score']\n",
    "            })\n",
    "        \n",
    "        overlap_df = pd.DataFrame(overlap_list)\n",
    "        overlap_df = overlap_df.sort_values('Original_P_adj')\n",
    "        \n",
    "        # Display top 10\n",
    "        print(overlap_df.head(10).to_string(index=False))\n",
    "        \n",
    "        return {\n",
    "            'overlap_df': overlap_df,\n",
    "            'orig_terms': orig_terms,\n",
    "            'put_terms': put_terms,\n",
    "            'overlap_terms': overlap_terms,\n",
    "            'overlap_count': len(overlap_terms),\n",
    "            'overlap_pct': overlap_pct\n",
    "        }\n",
    "    else:\n",
    "        print(\"No overlapping terms found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f24d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib-venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca059aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib_venn\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "for library in libraries:\n",
    "    comparison = compare_enrichment_results(\n",
    "        original_enrichment,\n",
    "        putative_enrichment,\n",
    "        library\n",
    "    )\n",
    "    \n",
    "    if comparison is not None:\n",
    "        comparison_results[library] = comparison\n",
    "\n",
    "def plot_venn_diagram(comparison, library_name):\n",
    "    \"\"\"Create Venn diagram showing overlap of enriched terms.\"\"\"\n",
    "    if comparison is None:\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    venn = matplotlib_venn.venn2(\n",
    "        subsets=(\n",
    "            len(comparison['orig_terms']) - len(comparison['overlap_terms']),\n",
    "            len(comparison['put_terms']) - len(comparison['overlap_terms']),\n",
    "            len(comparison['overlap_terms'])\n",
    "        ),\n",
    "        set_labels=('Original Disease Genes', 'Putative Disease Genes'),\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Customize colors\n",
    "    if venn.get_patch_by_id('10'):\n",
    "        venn.get_patch_by_id('10').set_color('lightblue')\n",
    "        venn.get_patch_by_id('10').set_alpha(0.6)\n",
    "    if venn.get_patch_by_id('01'):\n",
    "        venn.get_patch_by_id('01').set_color('lightcoral')\n",
    "        venn.get_patch_by_id('01').set_alpha(0.6)\n",
    "    if venn.get_patch_by_id('11'):\n",
    "        venn.get_patch_by_id('11').set_color('lightgreen')\n",
    "        venn.get_patch_by_id('11').set_alpha(0.7)\n",
    "    \n",
    "    plt.title(f'Enriched Terms Overlap\\n{library_name}', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"\\nGenerating Venn diagrams...\")\n",
    "\n",
    "for library in comparison_results.keys():\n",
    "    fig = plot_venn_diagram(comparison_results[library], library)\n",
    "    if fig:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY: ENRICHMENT OVERLAP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for library in libraries:\n",
    "    if library in original_enrichment and not original_enrichment[library].empty:\n",
    "        orig_count = len(original_enrichment[library])\n",
    "    else:\n",
    "        orig_count = 0\n",
    "    \n",
    "    if library in putative_enrichment and not putative_enrichment[library].empty:\n",
    "        put_count = len(putative_enrichment[library])\n",
    "    else:\n",
    "        put_count = 0\n",
    "    \n",
    "    if library in comparison_results:\n",
    "        overlap_count = comparison_results[library]['overlap_count']\n",
    "        overlap_pct = comparison_results[library]['overlap_pct']\n",
    "    else:\n",
    "        overlap_count = 0\n",
    "        overlap_pct = 0.0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Library': library.replace('_', ' '),\n",
    "        'Original Genes\\nEnriched Terms': orig_count,\n",
    "        'Putative Genes\\nEnriched Terms': put_count,\n",
    "        'Overlapping\\nTerms': overlap_count,\n",
    "        'Overlap %': f\"{overlap_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea2e7f",
   "metadata": {},
   "source": [
    "## 4. Drug repurposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03106f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Annet ?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
